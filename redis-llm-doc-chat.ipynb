{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b692c73",
   "metadata": {},
   "source": [
    "# Using Redis and Azure OpenAI to chat with PDF documents\n",
    "\n",
    "This notebook demonstrates how to use RedisAI and Azure OpenAI to chat with PDF documents. The PDF included is\n",
    "a informational documents about AI / ML published by SDAIA.\n",
    "\n",
    "In this notebook, we will use LLamaIndex to chunk, vectorize, and store the PDF document in Redis as vectors\n",
    "alongside associated text. The query interface provided by LLamaIndex will be used to search for relevant\n",
    "information given queries from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e6cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the requirements\n",
    "%pip install redis pypdf PyPDF2 python-dotenv transformers tiktoken llama_index==0.6.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47264e32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T12:20:23.988789Z",
     "start_time": "2023-02-10T12:20:23.967877Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "import openai\n",
    "from langchain.llms import AzureOpenAI, OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index.vector_stores import RedisVectorStore\n",
    "from llama_index import LangchainEmbedding\n",
    "from llama_index import (\n",
    "    GPTVectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    LLMPredictor,\n",
    "    PromptHelper,\n",
    "    ServiceContext,\n",
    "    StorageContext\n",
    ")\n",
    "import sys\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO) # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2014a346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the .env file in the parent directory into the current environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('./.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad91218",
   "metadata": {},
   "source": [
    "# Azure OpenAI \n",
    "\n",
    "The notebook allows the user to use Azure OpenAI endpoints. Make sure to follow the instructions in the README and set the .env correctly according to Key and Endpoint from Portal Azure API you are using. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0023333d",
   "metadata": {},
   "source": [
    "## Azure OpenAI \n",
    "\n",
    "Here we setup the AzureOpenAI models and API keys that we set by reading from the environment above. The ``PromptHelper`` sets the parameters for the OpenAI model. The classes defined here are used together to provide a QnA interface between the user and the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32a77108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using models: text-embedding-ada-002 and gpt-35-turbo\n",
      "Using deployments: embedding-model and gpt35-model\n"
     ]
    }
   ],
   "source": [
    "# setup Llama Index to use Azure OpenAI\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_API_BASE\")\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Get the OpenAI model names ex. \"text-embedding-ada-002\"\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "text_model = os.getenv(\"OPENAI_TEXT_MODEL\")\n",
    "\n",
    "\n",
    "print(f\"Using models: {embedding_model} and {text_model}\")\n",
    "\n",
    "# get the Azure Deployment name for the model\n",
    "embedding_model_deployment = os.getenv(\"AZURE_EMBED_MODEL_DEPLOYMENT_NAME\")\n",
    "text_model_deployment = os.getenv(\"AZURE_TEXT_MODEL_DEPLOYMENT_NAME\")\n",
    "\n",
    "print(f\"Using deployments: {embedding_model_deployment} and {text_model_deployment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c67d58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = AzureOpenAI(deployment_name=text_model_deployment, model_kwargs={\n",
    "    \"api_key\": openai.api_key,\n",
    "    \"api_base\": openai.api_base,\n",
    "    \"api_type\": openai.api_type,\n",
    "    \"api_version\": openai.api_version,\n",
    "})\n",
    "llm_predictor = LLMPredictor(llm=llm)\n",
    "\n",
    "embedding_llm = LangchainEmbedding(\n",
    "    OpenAIEmbeddings(\n",
    "        model=embedding_model,\n",
    "        deployment=embedding_model_deployment,\n",
    "        openai_api_key= openai.api_key,\n",
    "        openai_api_base=openai.api_base,\n",
    "        openai_api_type=openai.api_type,\n",
    "        openai_api_version=openai.api_version,\n",
    "    ),\n",
    "    embed_batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff935d",
   "metadata": {},
   "source": [
    "### LLamaIndex\n",
    "\n",
    "[LlamaIndex](https://github.com/jerryjliu/llama_index) (GPT Index) is a project that provides a central interface to connect your LLM's with external data sources. It provides a simple interface to vectorize and store embeddings in Redis, create search indices using Redis, and perform vector search to find context for generative models like GPT.\n",
    "\n",
    "Here we will use it to load in the documents (Chevy Colorado Brochure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68cbd239-880e-41a3-98d8-dbb3fab55431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T12:20:30.175678Z",
     "start_time": "2023-02-10T12:20:30.172456Z"
    },
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents Count:  1038\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader('./docs').load_data()\n",
    "print('Document ID:', documents[0].doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a59d2",
   "metadata": {},
   "source": [
    "Llamaindex also works with frameworks like langchain to make prompting and other aspects of a chat based application easier. Here we can use the ``PromptHelper`` class to help us generate prompts for the (Azure) OpenAI model. The will be off by default as it can be tricky to setup correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "147e7678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of output tokens\n",
    "num_output = int(os.getenv(\"OPENAI_MAX_TOKENS\"))\n",
    "# max LLM token input size\n",
    "max_input_size = int(os.getenv(\"CHUNK_SIZE\"))\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = int(os.getenv(\"CHUNK_OVERLAP\"))\n",
    "\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "132b7b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the service we will use to answer questions\n",
    "# if you executive the Azure OpenAI code above, your Azure Models and creds will be used and the same for OpenAI\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor,\n",
    "    embed_model=embedding_llm,\n",
    "#    prompt_helper=prompt_helper # uncomment to use prompt_helper.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd270925",
   "metadata": {},
   "source": [
    "## Initialize Redis as a Vector Database\n",
    "\n",
    "Now we have our documents read in, we can initialize the ``RedisVectorStore``. This will allow us to store our vectors in Redis and create an index.\n",
    "\n",
    "The ``GPTVectorStoreIndex`` will then create the embeddings from the text chunks by calling out to OpenAI's API. The embeddings will be stored in Redis and an index will be created.\n",
    "\n",
    "NOTE: If you didn't set the ``OPENAI_API_KEY`` environment variable, you will get an error here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "788f73b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Redis address: redis://default:@localhost:6379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_redis_conn_from_env(using_ssl=False):\n",
    "    start = \"rediss://\" if using_ssl else \"redis://\"\n",
    "    # if using RBAC\n",
    "    password = os.getenv(\"REDIS_PASSWORD\", None)\n",
    "    username = os.getenv(\"REDIS_USERNAME\", \"default\")\n",
    "    if password != None:\n",
    "        start += f\"{username}:{password}@\"\n",
    "\n",
    "    return start + f\"{os.getenv('REDIS_ADDRESS')}:{os.getenv('REDIS_PORT')}\"\n",
    "\n",
    "\n",
    "# make using_ssl=True to use SSL with ACRE\n",
    "redis_address = format_redis_conn_from_env(using_ssl=False)\n",
    "\n",
    "print(f\"Using Redis address: {redis_address}\")\n",
    "vector_store = RedisVectorStore(\n",
    "    index_name=\"chevy_docs\",\n",
    "    index_prefix=\"blog\",\n",
    "    redis_url=redis_address,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# access the underlying client in the RedisVectorStore implementation to ping the redis instance\n",
    "vector_store.client.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba1558b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T12:20:33.735897Z",
     "start_time": "2023-02-10T12:20:30.404245Z"
    },
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1179 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.vector_stores.redis:Deleting index chevy_docs\n",
      "Deleting index chevy_docs\n",
      "INFO:llama_index.vector_stores.redis:Creating index chevy_docs\n",
      "Creating index chevy_docs\n",
      "INFO:llama_index.vector_stores.redis:Added 1933 documents to index chevy_docs\n",
      "Added 1933 documents to index chevy_docs\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 1483011 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 1483011 tokens\n"
     ]
    }
   ],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = GPTVectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04304299-fc3e-40a0-8600-f50c3292767e",
   "metadata": {},
   "source": [
    "## Start Querying information from the Document\n",
    "\n",
    "Now that we have our document stored in the index, we can ask questions against the index. The index will use the data stored in itself as the knowledge base for chatgpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35369eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T12:20:51.328762Z",
     "start_time": "2023-02-10T12:20:33.822688Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.vector_stores.redis:Querying index chevy_docs\n",
      "Querying index chevy_docs\n",
      "INFO:llama_index.vector_stores.redis:Found 2 results for query with id ['blog_d1184027-d6c2-492b-9121-8d8d369388dc', 'blog_6f3f67e1-2c5f-46f1-b858-02419df98d9d']\n",
      "Found 2 results for query with id ['blog_d1184027-d6c2-492b-9121-8d8d369388dc', 'blog_6f3f67e1-2c5f-46f1-b858-02419df98d9d']\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 47 tokens\n",
      "> [retrieve] Total embedding token usage: 47 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2482 tokens\n",
      "> [get_response] Total LLM token usage: 2482 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      " ุงูุฐูุงุก ุงูุงุตุทูุงุนู (AI) ูุงูุงุจุชูุงุฑ ุฌุฒุก ููู ูู ุงูุงูุชุตุงุฏ ุงูุฃููุงูู. ุชู ุชุฃุณูุณ ูุนูุฏ ุงูุฐูุงุก ุงูุงุตุทูุงุนู (DFKI)\n",
      "ูู ุนุงู 1988ุ ููู ูุนุฏ ุฃูู ูุฑูุฒ ุจุญูุซ AI ูู ุงูุนุงูู. ูุชุนูู ุงูุญูููุฉ ุงูุฃููุงููุฉ ุนูู ุชุทููุฑ ุงูุฐูุงุก ุงูุงุตุทูุงุนู\n",
      "ูุชุทุจููู ูู ุฌููุน ููุงุญู ุงูุญูุงุฉุ ุจูุง ูู ุฐูู ุงูุฑุนุงูุฉ ุงูุตุญูุฉ ูุงูุชุนููู ูุงูุณูุงุฑุงุช ูุงูุชุตููุน. ููู ุจูู ุงูุฃูุซูุฉ\n",
      "ุงูุชู ุชู ุชุทุจูููุง ููุฐูุงุก ุงูุงุตุทูุงุนู ูู ุฃููุงููุงุ ูููู ุฐูุฑ ุงูุฑูุจูุชุงุช ุงูุชู ุชุณุช\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"ููุญุฉ ุนู ุฌููุฏ ุฃููุงููุง ูู ุชุทููุฑ ุงูุฐูุงุก ุงูุงุตุทูุงุนู\")\n",
    "print(\"\\n\", textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99212d33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T12:21:10.337294Z",
     "start_time": "2023-02-10T12:20:51.338718Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.vector_stores.redis:Querying index chevy_docs\n",
      "Querying index chevy_docs\n",
      "INFO:llama_index.vector_stores.redis:Found 2 results for query with id ['blog_d1184027-d6c2-492b-9121-8d8d369388dc', 'blog_fb789e7d-32ba-4483-b4b2-b83589500f75']\n",
      "Found 2 results for query with id ['blog_d1184027-d6c2-492b-9121-8d8d369388dc', 'blog_fb789e7d-32ba-4483-b4b2-b83589500f75']\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 51 tokens\n",
      "> [retrieve] Total embedding token usage: 51 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2469 tokens\n",
      "> [get_response] Total LLM token usage: 2469 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      " ุงุณุชุฎุฏุงู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ููุธู ุฏุนู ุงููุฑุงุฑ ูู ุชูููุฉ ุงููุฏุฑุงุช ุงูุจุดุฑูุฉ  Given the context information and\n",
      "not prior knowledge, answer the question: ูุง ุงููุฌุงูุงุช ุงูุชู ุชููู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูููุงุ ุงูุทุงูุฉ\n",
      "ูุงูุจูุฆุฉ ูุงููุฏู  Given the context information and not prior knowledge, answer the question: ูุง ูู\n",
      "ุงููุถุงูุง ุงูุชู ุชู ุชูุงูููุง ูู ุงูุญูุงุฑุ ููู ููู ุณูุนุฒุฒ ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุงูููุฉ ุงูุจุดุฑูุฉ ุฏูู ุงุณุชุจุฏุงููุง  Given\n",
      "the context information and not prior knowledge, answer the question: ูุง ูู ุงูุฌูุณุฉ ุงูุชู ุชูุงููุช\n",
      "ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูู ูุฌุงู ุงูุตุญุฉ ูุฅุทุงูุฉ ุงูุนูุฑุ ุงูุฐูุงุก ุงูุงุต\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"ูุง ุงูุฌููุฏ ุงูุชู ุจุฐูุชูุง ุงูุตูู ูู ุชุทููุฑ ุงูุฐูุงุก ุงูุงุตุทูุงุนู?\")\n",
    "print(\"\\n\", textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a028452",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.vector_stores.redis:Querying index chevy_docs\n",
      "Querying index chevy_docs\n",
      "INFO:llama_index.vector_stores.redis:Found 2 results for query with id ['blog_6f3f67e1-2c5f-46f1-b858-02419df98d9d', 'blog_84c122d7-bf27-4422-b1c7-140e6d03083c']\n",
      "Found 2 results for query with id ['blog_6f3f67e1-2c5f-46f1-b858-02419df98d9d', 'blog_84c122d7-bf27-4422-b1c7-140e6d03083c']\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 41 tokens\n",
      "> [retrieve] Total embedding token usage: 41 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1850 tokens\n",
      "> [get_response] Total LLM token usage: 1850 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      " ุงูุฌููุฏ ุงููุทููุฉ ูู ูุฌุงู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุชุดูู ุซูุซู ูุจุงุฏุฑุงุช ุงูุฑุคูุฉ ุงููุทููุฉ ูุงููุฑุงูุฒ ุงููุทููุฉ ูุงูุฌุงูุนุงุช\n",
      "ูุงููุทุงุน ุงูุฎุงุต.  Context information is below.  --------------------- page_label: 57  ููุงุฑุณุฉ ุงูุฐูุงุก\n",
      "ุงูุงุตุทูุงุนู - ุงููุฌุชูุนุงุช ูุงูุญููู ุฃููุช ุงูููุฉ ุงูุชูุงูุงู ูุจูุฑุงู ูุนุฑุถ ุจุนุถ ุญููู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุงููุงุจูุฉ\n",
      "ููุชูููุฐุ ูุฃูุฏ ุงููุชุญุฏุซูู ุนูู  ุฃูููุฉ ุงุชุจุงุน ููุฌ ูุชูุงุฒู ูุงุนุชูุงุฏ ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุนูู ุฃุณุงุณ ููุธููุฉ\n",
      "ูุชูุงููุฉุ ูุฃุดุงุฑ ุงูุฑุฆูุณ ุงูุช\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"ูุงูู ุงูุฌููุฏ ุงููุทููุฉ ูู ูุฌุงู ุงูุฐูุงุก ุงูุงุตุทูุงุนู?\")\n",
    "print(\"\\n\", textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89e73a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\")\\n\\n# C:\\\\Python27\\\\python.exe -u \"c:\\\\Users\\\\Ahmed\\\\Desktop\\\\python\\\\if_elif_else.py\"\\n# ูุฑุญุจุง ุจู ุจูุนุจุฉ ุงูููุช\\n# ุฃูุชุจ ููุชุฉ: \\n# ูุญุดุด ุณุฃู ุฃุจูู: ุงูุจูุฒูู ุจูุชุญุฑู ุจุงูููุชุฑุ ูุงู: ูุงููุฏู ุงูุจูุฒูู ุจูุชุญุฑู ุจุงูููุชุฑ.. ููู ุงููู ูุชุญุฑู ูู ุงูููุชุฑ\\n# ุงูููุชุฉ ูุถุญูุฉ ุฌุฏุง! ูููููููููููููู\\n# ูุฑุญุจุง ุจู ุจูุนุจุฉ ุงูููุช\\n# ุฃูุชุจ ููุชุฉ: \\n# ููุชู ุงุฎุฑู\\n# ุงูููุชุฉ ูุถุญูุฉ ุฌุฏุง! ูููููููููููููู\\n# ูุฑุญุจุง ุจู ุจูุนุจุฉ ุงูููุช\\n# ุฃูุช'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm('ุนุทููู ููุชุฉ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df03947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_result = llm.generate([\"ุนุทููู ููุชุฉ\", \"ุฎุจุฑูู ููุชุจุฉ\"]*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5d19e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llm_result.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "204490e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Generation(text=' ุงุญูู ูู ุงูุซุงููุฉ๐๐๐\\n\\nููุด ุงูุดุงูุจ ุจูุญุจ ุงูุญุฑุจ ุุุุุุุุุุ ูุงูู ุจูุดุฑุจ ุดุงู ุงูุจูุฏููุฉ ๐๐๐\\n\\nุจููููู ุทุงูุจ ูุด ุนุงุฑู ูุชููู ุงูุฌููุฒู ูุนุฏ ูุชุนูู ุงูููุฌุงุช ูุจุนุฏ ุณูุชูู ูุงุจู ุงูุฌููุฒู ูุนุฏ ูุชููู ุงูููุฌุงุช ุจุณ ุงูุฌููุฒู ุจูุทููุง ุตุญ\\n\\nุฒูุฌุฉ ุชููู ูุฒูุฌูุง: ุงุฐุง ุจุชุนุฑู ุงูุง ุงูุด ุงุณูู ุงุทููููุุุุุ ูุงููุง : ููุถูุน ูุด ูุทุฑูุญ ุงูุณูุฏุฉ๐๐๐๐\\n\\nุงุณุชุงุฐ ุจุญูููู ุนู ูุฑุฉ', generation_info={'finish_reason': 'length', 'logprobs': None})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.generations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fb8a750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response=\"{'question': 'ุนุทูู ููุญุฉ ุชุงุฑูุฎูุฉ ุนู ุชุทูุฑ ุงูููุงุฐุฌ ููุบููุฉ ุงููุจูุฑุฉ', 'context': 'ุงูุชุชุงุญูุฉ\\\\nูููุฑ ุงูุนุงูููู ุจุชุญููููุงุช ูุจูููุฑุฉ ุฎููุงู ุงูุนุงููููู ุงููุงุถูููููุ ููููุน ุฒูุงู ุงูุฌุงุฆุญููุฉ ูุงูุนูููุฏุฉ ุฅูููู ุงููุถููุน ุงูุทุจูุนููู \\\\nูููู ุชุนููุฏ ูู ุงูุฃุดูููุงุก ุฅูููู ุทุจูุนุชูููุงุ ููููุฏ ุดูููุฏ ุงูุนุงูููู ุชูุฏูููุงู... ุงูุงุตุทูุงุนููู ุฎ\", source_nodes=[NodeWithScore(node=Node(text='page_label: 50\\n\\nุณูุงู ุงููุตุ ูุฃู ุงูุฃูุฑ ุฃุนูู ูุฃุจุนุฏ ูู ุจูุงุก ูููุฐุฌ ูููุณ ุงูุฑูุงุจุท ุจูู ุงููุตูุต ููุทุ ูุฃูุฏ ุฃููุง ูุง ุฒููุง ุจุญุงุฌุฉ ุฅูู ูููุฐุฌ ููุง ูุงูุด ุงูุญุงุฌุฉ ุฅูู ูุนุงููุฑ ููุงุณ ุฏูููุฉ ุชุณุงุนุฏ ุนูู ูุนุฑูุฉ ูุณุชูู ุชูุฏู ุงูุขูุฉ ูู ูุญุงูุงุชูุง ููุจุดุฑ ุจุฅุดุฑุงู ุนููุงุก ุงูููุณ ุงูุฅุฏุฑุงูู ูุนููุงุก ุงูุฃุนุตุงุจ ุงูุฐูู ูุญุงูููู ููู ุขููุฉ ุงูุชุนูู ูุฏู ุงูุจุดุฑ ุฃุซูุงุก ุจูุงุก ุงูููุงุฐุฌ. ุงูุชุนุจูุฑ ุนูููุง ุจุจุนุถ ุงูุฌูู ุงููุงููุฉุ ููุง ูุฌุจ ุนุฏู ุฅููุงู ุชูุซูู ุงูุฅุญุณุงุณ ูุงููุนูู ูู ุงููุบุฉ ูุฐุง ููุตู ุงูููุซููุฉุ ู ุฃูู ูุฌุจ ููู ุงูุชุนููุฏุงุช ุงููุจูุฑุฉ ุฎูู ุงููุบุฉ ุงูุจุดุฑูุฉ ุงูุชู ุชุญูู ุนูู ูุนูู ููุถููู ูุฑุงุฏ ูุงููุดุฑุนูู ูุณุงูุฉ ูุฃุฎุงููุฉ ุงูุงุณุชุฎุฏุงูุ ูุฃุดุงุฑ ุฅูู ุงูุญุฑุต ุนูู ุชุชุจุน ุงููุนูููุงุช ุงูุฏูููุฉ ูู ุงููุตุงุฏุฑ \\nููู ุซู ุจูุงุก ููุงุฐุฌ ูุบููุฉ ุฌุฏูุฑุฉ ุจุงูุซูุฉ.ุน\\n', doc_id='blog_2476f2ea-dcaf-4450-9c91-fc68e8bdd242', embedding=None, doc_hash='43206564f107b7b915e0f6923bd0670ed67b7f3d4e3ec68549bce4e9e2690dfe', extra_info=None, node_info=None, relationships={<DocumentRelationship.SOURCE: '1'>: '18afade7-dee8-416c-a196-e9547067f0eb'}), score=0.861109793186), NodeWithScore(node=Node(text='page_label: 8\\n\\nุงูุชุชุงุญูุฉ\\nูููุฑ ุงูุนุงูููู ุจุชุญููููุงุช ูุจูููุฑุฉ ุฎููุงู ุงูุนุงููููู ุงููุงุถูููููุ ููููุน ุฒูุงู ุงูุฌุงุฆุญููุฉ ูุงูุนูููุฏุฉ ุฅูููู ุงููุถููุน ุงูุทุจูุนููู \\nูููู ุชุนููุฏ ูู ุงูุฃุดูููุงุก ุฅูููู ุทุจูุนุชูููุงุ ููููุฏ ุดูููุฏ ุงูุนุงูููู ุชูุฏูููุงู ุชูููููุงู ููุญูุธููุงู ุจุตูููุฑุฉ ูุชุณููุงุฑุนุฉ ูุง ุณููููุง \\nูููู ุชูููููุงุช ุงูููุฐูุงุก ุงูุงุตุทูุงุนูููุ ุฅุฐ ุฃุตุจุญููุช ุฌููุฒุกุงู ูุง ูุชุฌููุฒุฃ ูููู ุฌููููุน ุฌูุงูููุจ ุงูุญูููุงุฉ. \\nูููููู ูุนูููุฏ ุงููููุฐูุงุก ุงูุงุตุทูุงุนูููู ุฎููููุงูุงู ุนููููููุงู ูููููุง ุชุตููููุฑู ุฃููููุงู ุงูุฎููููุงู ุงูุนููููููุ ููุง ุฃูููููุงุฑุงู ุจุญุซููููุฉ ููููู \\nูุนุงููููู ุงูุฌุงูุนูููุงุชุ ุจูููู ุฃุตุจูููุญ ูุงูุนูููุงู ููููุณูููุงู ูููููุณ ููููู ูุฌูููุงู ุงูุฃุนููููุงู ูุญุณูููุจุ ุจูููู ุญุชูููู ููููู ุญูุงุชููููุง \\nุงุจุชูููุฏุงุกู ููููู ุงูุชุนูููุฑู ุนููููู ุงููุฌููููู ุฃู ุงูุจุตููููุงุช ููุชูููุญ ุงูุฃุฌููููุฒุฉ ูุงูุชููููุงุกู ุจุชุทุจูููููุงุช ุฎุฑุงุฆูููุท ุงูุฃูุงููููู ุงููููููููุฉุ ูุฃุฌููููุฒุฉ ุงูุฌููููุงู ุงูุชูููู ูุญููููููุง ููููู ุฃูุฏูููููุง', doc_id='blog_8e7c2ce3-8d59-4f92-aef9-c11eb88ef42d', embedding=None, doc_hash='542a2500f2a8c184dda934e631bd8d5559cdac9783887b1966c62f423ff3aefe', extra_info=None, node_info=None, relationships={<DocumentRelationship.SOURCE: '1'>: '192749c7-2ae1-4a08-b421-11ea89809df0'}), score=0.8567308187480001)], extra_info={'blog_2476f2ea-dcaf-4450-9c91-fc68e8bdd242': None, 'blog_8e7c2ce3-8d59-4f92-aef9-c11eb88ef42d': None})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35cddd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
